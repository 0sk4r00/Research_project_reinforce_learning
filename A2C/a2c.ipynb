{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "import csv\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(s_size, h_size)\n",
    "        self.layer2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden = F.relu(self.layer1(state))\n",
    "        action_probs = F.softmax(self.layer2(hidden), dim=1)\n",
    "        return action_probs\n",
    "\n",
    "    def act(self, state):\n",
    "        probabilities = self.forward(state)\n",
    "        distribution = Categorical(probabilities)\n",
    "        action = distribution.sample()\n",
    "        return action.item(), distribution.log_prob(action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, s_size, h_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(s_size, h_size)\n",
    "        self.output_layer = nn.Linear(h_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hidden = F.relu(self.input_layer(state))\n",
    "        state_value = self.output_layer(hidden)\n",
    "        return state_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(policy, value_function, max_steps, env):\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    state_values = []\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action, log_prob = policy.act(state)\n",
    "        value = value_function(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        state_values.append(value)\n",
    "        state = next_state\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return log_probs, rewards, state_values\n",
    "\n",
    "def calculate_discounted_returns(rewards, max_steps, gamma):\n",
    "    returns = deque(maxlen=max_steps)\n",
    "    n_steps = len(rewards)\n",
    "    \n",
    "    for step in range(n_steps)[::-1]:\n",
    "        disc_return = (returns[0] if len(returns) > 0 else 0)\n",
    "        returns.appendleft(rewards[step] + gamma * disc_return)\n",
    "    return returns\n",
    "\n",
    "def standardise_returns(returns):\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    return returns\n",
    "\n",
    "def optimise_policy(policy_optimizer, log_probs, returns, state_values):\n",
    "    state_values = torch.stack(state_values).squeeze()\n",
    "    advantages = returns - state_values.detach()\n",
    "    advantages = torch.tensor(advantages).to(device)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, advantage in zip(log_probs, advantages):\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "def optimise_value_function(value_optimizer, returns, state_values):\n",
    "    state_values = torch.stack(state_values).squeeze()\n",
    "    value_loss = F.mse_loss(state_values, returns)\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, env, num_episodes=100, max_steps=1000):\n",
    "    all_rewards = []\n",
    "    all_steps = []\n",
    "    \n",
    "    with open('a2c_evaluation_results.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Episode', 'Reward', 'Steps'])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action, _ = policy.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        all_steps.append(step + 1)\n",
    "        \n",
    "        # Save episode results\n",
    "        with open('a2c_evaluation_results.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([i_episode + 1, episode_reward, step + 1])\n",
    "    \n",
    "    avg_reward = np.mean(all_rewards)\n",
    "    avg_steps = np.mean(all_steps)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    \n",
    "    print(f\"\\nEvaluation over {num_episodes} episodes:\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f} Â± {std_reward:.2f}\")\n",
    "    print(f\"Average Episode Length: {avg_steps:.2f}\")\n",
    "    \n",
    "    return avg_reward, avg_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Create CSV file for training progress\n",
    "    with open('a2c_training_rewards.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Episode', 'Reward', 'Mean_100'])\n",
    "\n",
    "    # Environment setup\n",
    "    env = gym.make('LunarLander-v3')\n",
    "    s_size = env.observation_space.shape[0]\n",
    "    a_size = env.action_space.n\n",
    "\n",
    "    # Hyperparameters\n",
    "    h_size = 256\n",
    "    max_steps = 1000\n",
    "    gamma = 0.99\n",
    "    lr = 1e-3\n",
    "    num_episodes = 3000\n",
    "\n",
    "    # Initialize networks\n",
    "    policy = PolicyNetwork(s_size, a_size, h_size).to(device)\n",
    "    value_function = ValueNetwork(s_size, h_size).to(device)\n",
    "    policy_optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    value_optimizer = optim.Adam(value_function.parameters(), lr=lr)\n",
    "\n",
    "    # Training tracking\n",
    "    recent_scores = deque(maxlen=100)\n",
    "    best_mean_reward = float('-inf')\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Generate trajectory\n",
    "        log_probs, rewards, state_values = generate_trajectory(policy, value_function, max_steps, env)\n",
    "        episode_score = sum(rewards)\n",
    "        recent_scores.append(episode_score)\n",
    "        \n",
    "        # Calculate returns and update networks\n",
    "        returns = calculate_discounted_returns(rewards, max_steps, gamma)\n",
    "        standardised_returns = standardise_returns(returns)\n",
    "        optimise_value_function(value_optimizer, standardised_returns, state_values)\n",
    "        optimise_policy(policy_optimizer, log_probs, standardised_returns, state_values)\n",
    "\n",
    "        # Calculate mean reward\n",
    "        mean_reward = np.mean(list(recent_scores))\n",
    "\n",
    "        # Save episode results\n",
    "        with open('a2c_training_rewards.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([episode + 1, episode_score, mean_reward])\n",
    "\n",
    "        # Save best model\n",
    "        if mean_reward > best_mean_reward and len(recent_scores) == 100:\n",
    "            best_mean_reward = mean_reward\n",
    "            torch.save({\n",
    "                'policy_state_dict': policy.state_dict(),\n",
    "                'value_state_dict': value_function.state_dict()\n",
    "            }, 'best_a2c_model.pt')\n",
    "            print(f\"\\nNew best model saved with mean reward: {best_mean_reward:.2f}\")\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_score:.2f}, Mean (100): {mean_reward:.2f}\")\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'policy_state_dict': policy.state_dict(),\n",
    "        'value_state_dict': value_function.state_dict()\n",
    "    }, 'final_a2c_model.pt')\n",
    "    print(\"\\nTraining completed. Final model saved.\")\n",
    "\n",
    "    # Load best model and evaluate\n",
    "    print(\"\\nEvaluating best model...\")\n",
    "    checkpoint = torch.load('best_a2c_model.pt')\n",
    "    policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "    value_function.load_state_dict(checkpoint['value_state_dict'])\n",
    "    evaluate(policy, env)\n",
    "\n",
    "    env.close()\n",
    "    return policy, value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
