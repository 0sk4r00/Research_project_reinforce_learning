{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Reward: -204.61, Mean (100): -198.02\n",
      "Episode 20, Reward: 35.20, Mean (100): -208.29\n",
      "Episode 30, Reward: -281.48, Mean (100): -200.35\n",
      "Episode 40, Reward: -380.58, Mean (100): -186.87\n",
      "Episode 50, Reward: -125.44, Mean (100): -173.40\n",
      "Episode 60, Reward: -158.63, Mean (100): -172.91\n",
      "Episode 70, Reward: -213.29, Mean (100): -182.46\n",
      "Episode 80, Reward: -279.97, Mean (100): -181.46\n",
      "Episode 90, Reward: -430.32, Mean (100): -186.25\n",
      "New best model saved with mean reward: -187.16\n",
      "Episode 100, Reward: -56.29, Mean (100): -187.16\n",
      "New best model saved with mean reward: -186.68\n",
      "New best model saved with mean reward: -185.80\n",
      "Episode 110, Reward: -226.77, Mean (100): -186.37\n",
      "New best model saved with mean reward: -182.52\n",
      "New best model saved with mean reward: -180.17\n",
      "New best model saved with mean reward: -180.03\n",
      "New best model saved with mean reward: -178.21\n",
      "New best model saved with mean reward: -176.96\n",
      "New best model saved with mean reward: -176.38\n",
      "Episode 120, Reward: -105.52, Mean (100): -177.79\n",
      "New best model saved with mean reward: -175.92\n",
      "New best model saved with mean reward: -174.65\n",
      "Episode 130, Reward: -285.07, Mean (100): -174.68\n",
      "New best model saved with mean reward: -174.02\n",
      "New best model saved with mean reward: -173.72\n",
      "New best model saved with mean reward: -173.62\n",
      "New best model saved with mean reward: -173.44\n",
      "New best model saved with mean reward: -172.15\n",
      "Episode 140, Reward: -104.58, Mean (100): -172.15\n",
      "New best model saved with mean reward: -172.14\n",
      "New best model saved with mean reward: -170.78\n",
      "New best model saved with mean reward: -169.84\n",
      "New best model saved with mean reward: -169.61\n",
      "Episode 150, Reward: -50.89, Mean (100): -169.61\n",
      "New best model saved with mean reward: -168.95\n",
      "New best model saved with mean reward: -168.08\n",
      "Episode 160, Reward: -71.37, Mean (100): -168.08\n",
      "New best model saved with mean reward: -166.91\n",
      "New best model saved with mean reward: -164.94\n",
      "New best model saved with mean reward: -163.34\n",
      "New best model saved with mean reward: -161.68\n",
      "New best model saved with mean reward: -160.51\n",
      "Episode 170, Reward: -96.80, Mean (100): -160.51\n",
      "New best model saved with mean reward: -159.40\n",
      "New best model saved with mean reward: -155.94\n",
      "New best model saved with mean reward: -154.93\n",
      "Episode 180, Reward: -118.32, Mean (100): -154.93\n",
      "New best model saved with mean reward: -150.72\n",
      "New best model saved with mean reward: -148.90\n",
      "New best model saved with mean reward: -148.57\n",
      "New best model saved with mean reward: -145.49\n",
      "Episode 190, Reward: -47.61, Mean (100): -145.49\n",
      "New best model saved with mean reward: -144.95\n",
      "New best model saved with mean reward: -140.74\n",
      "New best model saved with mean reward: -140.73\n",
      "New best model saved with mean reward: -137.94\n",
      "New best model saved with mean reward: -135.50\n",
      "New best model saved with mean reward: -135.33\n",
      "New best model saved with mean reward: -135.09\n",
      "Episode 200, Reward: -102.57, Mean (100): -135.56\n",
      "New best model saved with mean reward: -135.03\n",
      "New best model saved with mean reward: -134.77\n",
      "New best model saved with mean reward: -131.46\n",
      "New best model saved with mean reward: -129.28\n",
      "New best model saved with mean reward: -128.43\n",
      "New best model saved with mean reward: -127.21\n",
      "Episode 210, Reward: -104.38, Mean (100): -127.21\n",
      "New best model saved with mean reward: -124.56\n",
      "New best model saved with mean reward: -124.46\n",
      "New best model saved with mean reward: -124.15\n",
      "New best model saved with mean reward: -123.21\n",
      "New best model saved with mean reward: -122.88\n",
      "New best model saved with mean reward: -121.36\n",
      "Episode 220, Reward: 46.79, Mean (100): -121.36\n",
      "New best model saved with mean reward: -121.05\n",
      "New best model saved with mean reward: -119.12\n",
      "New best model saved with mean reward: -118.64\n",
      "New best model saved with mean reward: -118.04\n",
      "New best model saved with mean reward: -116.08\n",
      "New best model saved with mean reward: -114.43\n",
      "Episode 230, Reward: -401.06, Mean (100): -121.64\n",
      "Episode 240, Reward: -99.90, Mean (100): -123.82\n",
      "Episode 250, Reward: -168.56, Mean (100): -129.86\n",
      "Episode 260, Reward: -129.94, Mean (100): -133.91\n",
      "Episode 270, Reward: -166.74, Mean (100): -129.94\n",
      "Episode 280, Reward: -36.90, Mean (100): -130.45\n",
      "Episode 290, Reward: -62.04, Mean (100): -127.87\n",
      "Episode 300, Reward: -35.93, Mean (100): -127.61\n",
      "Episode 310, Reward: -15.15, Mean (100): -123.35\n",
      "Episode 320, Reward: -136.08, Mean (100): -125.02\n",
      "Episode 330, Reward: -46.71, Mean (100): -124.90\n",
      "Episode 340, Reward: -26.07, Mean (100): -116.70\n",
      "New best model saved with mean reward: -114.29\n",
      "New best model saved with mean reward: -113.36\n",
      "New best model saved with mean reward: -110.22\n",
      "New best model saved with mean reward: -109.31\n",
      "Episode 350, Reward: -77.16, Mean (100): -109.31\n",
      "New best model saved with mean reward: -108.38\n",
      "New best model saved with mean reward: -104.18\n",
      "New best model saved with mean reward: -102.60\n",
      "New best model saved with mean reward: -100.91\n",
      "New best model saved with mean reward: -99.82\n",
      "New best model saved with mean reward: -97.49\n",
      "New best model saved with mean reward: -94.37\n",
      "New best model saved with mean reward: -94.19\n",
      "New best model saved with mean reward: -93.49\n",
      "Episode 360, Reward: -59.31, Mean (100): -93.49\n",
      "New best model saved with mean reward: -91.89\n",
      "New best model saved with mean reward: -91.62\n",
      "New best model saved with mean reward: -89.84\n",
      "New best model saved with mean reward: -89.57\n",
      "New best model saved with mean reward: -88.71\n",
      "New best model saved with mean reward: -87.61\n",
      "New best model saved with mean reward: -86.76\n",
      "New best model saved with mean reward: -85.82\n",
      "Episode 370, Reward: 37.44, Mean (100): -85.82\n",
      "New best model saved with mean reward: -84.90\n",
      "New best model saved with mean reward: -81.59\n",
      "New best model saved with mean reward: -81.10\n",
      "New best model saved with mean reward: -79.38\n",
      "New best model saved with mean reward: -78.88\n",
      "Episode 380, Reward: -131.25, Mean (100): -79.83\n",
      "New best model saved with mean reward: -77.31\n",
      "New best model saved with mean reward: -76.70\n",
      "New best model saved with mean reward: -76.00\n",
      "New best model saved with mean reward: -75.69\n",
      "New best model saved with mean reward: -73.92\n",
      "New best model saved with mean reward: -73.52\n",
      "Episode 390, Reward: -21.87, Mean (100): -73.52\n",
      "New best model saved with mean reward: -71.22\n",
      "New best model saved with mean reward: -69.41\n",
      "New best model saved with mean reward: -67.80\n",
      "Episode 400, Reward: -18.17, Mean (100): -69.12\n",
      "New best model saved with mean reward: -67.68\n",
      "New best model saved with mean reward: -67.51\n",
      "Episode 410, Reward: -163.82, Mean (100): -71.51\n",
      "New best model saved with mean reward: -66.85\n",
      "New best model saved with mean reward: -65.45\n",
      "New best model saved with mean reward: -64.48\n",
      "Episode 420, Reward: -39.35, Mean (100): -64.48\n",
      "New best model saved with mean reward: -64.33\n",
      "New best model saved with mean reward: -62.34\n",
      "New best model saved with mean reward: -60.59\n",
      "New best model saved with mean reward: -58.79\n",
      "New best model saved with mean reward: -55.73\n",
      "New best model saved with mean reward: -55.54\n",
      "New best model saved with mean reward: -53.50\n",
      "New best model saved with mean reward: -53.16\n",
      "New best model saved with mean reward: -52.78\n",
      "Episode 430, Reward: -8.64, Mean (100): -52.78\n",
      "New best model saved with mean reward: -52.49\n",
      "New best model saved with mean reward: -51.96\n",
      "New best model saved with mean reward: -51.11\n",
      "New best model saved with mean reward: -51.02\n",
      "New best model saved with mean reward: -49.44\n",
      "New best model saved with mean reward: -48.94\n",
      "New best model saved with mean reward: -48.17\n",
      "New best model saved with mean reward: -47.71\n",
      "New best model saved with mean reward: -47.48\n",
      "Episode 440, Reward: -2.89, Mean (100): -47.48\n",
      "New best model saved with mean reward: -46.97\n",
      "New best model saved with mean reward: -46.64\n",
      "New best model saved with mean reward: -44.70\n",
      "New best model saved with mean reward: -44.36\n",
      "Episode 450, Reward: -92.25, Mean (100): -45.32\n",
      "Episode 460, Reward: -64.40, Mean (100): -52.04\n",
      "Episode 470, Reward: 10.38, Mean (100): -53.68\n",
      "Episode 480, Reward: 30.58, Mean (100): -49.21\n",
      "New best model saved with mean reward: -44.13\n",
      "New best model saved with mean reward: -43.97\n",
      "Episode 490, Reward: -5.67, Mean (100): -43.97\n",
      "New best model saved with mean reward: -43.45\n",
      "New best model saved with mean reward: -43.10\n",
      "New best model saved with mean reward: -42.97\n",
      "New best model saved with mean reward: -42.31\n",
      "New best model saved with mean reward: -42.13\n",
      "New best model saved with mean reward: -40.85\n",
      "New best model saved with mean reward: -40.74\n",
      "New best model saved with mean reward: -40.59\n",
      "Episode 500, Reward: -3.58, Mean (100): -40.59\n",
      "New best model saved with mean reward: -40.08\n",
      "New best model saved with mean reward: -39.85\n",
      "New best model saved with mean reward: -38.45\n",
      "New best model saved with mean reward: -36.85\n",
      "New best model saved with mean reward: -35.89\n",
      "New best model saved with mean reward: -33.97\n",
      "New best model saved with mean reward: -33.42\n",
      "New best model saved with mean reward: -33.20\n",
      "New best model saved with mean reward: -31.42\n",
      "Episode 510, Reward: 14.71, Mean (100): -31.42\n",
      "New best model saved with mean reward: -31.04\n",
      "New best model saved with mean reward: -30.50\n",
      "New best model saved with mean reward: -30.14\n",
      "New best model saved with mean reward: -29.82\n",
      "New best model saved with mean reward: -29.61\n",
      "New best model saved with mean reward: -29.16\n",
      "Episode 520, Reward: 5.79, Mean (100): -29.16\n",
      "New best model saved with mean reward: -28.85\n",
      "New best model saved with mean reward: -28.14\n",
      "New best model saved with mean reward: -27.73\n",
      "New best model saved with mean reward: -27.47\n",
      "New best model saved with mean reward: -26.92\n",
      "New best model saved with mean reward: -25.74\n",
      "New best model saved with mean reward: -25.20\n",
      "Episode 530, Reward: -14.42, Mean (100): -25.26\n",
      "New best model saved with mean reward: -24.98\n",
      "New best model saved with mean reward: -24.13\n",
      "New best model saved with mean reward: -23.94\n",
      "New best model saved with mean reward: -23.47\n",
      "New best model saved with mean reward: -23.47\n",
      "New best model saved with mean reward: -22.52\n",
      "Episode 540, Reward: -22.67, Mean (100): -24.84\n",
      "New best model saved with mean reward: -21.01\n",
      "New best model saved with mean reward: -20.84\n",
      "Episode 550, Reward: -75.81, Mean (100): -20.84\n",
      "New best model saved with mean reward: -18.52\n",
      "New best model saved with mean reward: -17.44\n",
      "New best model saved with mean reward: -17.41\n",
      "Episode 560, Reward: -134.68, Mean (100): -18.12\n",
      "Episode 570, Reward: -132.79, Mean (100): -26.87\n",
      "Episode 580, Reward: 57.77, Mean (100): -24.03\n",
      "Episode 590, Reward: 30.80, Mean (100): -22.93\n",
      "Episode 600, Reward: -43.49, Mean (100): -23.57\n",
      "Episode 610, Reward: -63.21, Mean (100): -26.77\n",
      "Episode 620, Reward: 32.59, Mean (100): -29.14\n",
      "Episode 630, Reward: -37.18, Mean (100): -30.71\n",
      "Episode 640, Reward: 39.32, Mean (100): -29.13\n",
      "Episode 650, Reward: 127.88, Mean (100): -32.45\n",
      "Episode 660, Reward: -182.02, Mean (100): -24.06\n",
      "New best model saved with mean reward: -15.17\n",
      "New best model saved with mean reward: -13.20\n",
      "New best model saved with mean reward: -11.59\n",
      "New best model saved with mean reward: -8.40\n",
      "New best model saved with mean reward: -7.11\n",
      "Episode 670, Reward: -4.22, Mean (100): -7.11\n",
      "New best model saved with mean reward: -4.14\n",
      "New best model saved with mean reward: -3.77\n",
      "New best model saved with mean reward: -3.45\n",
      "New best model saved with mean reward: -2.83\n",
      "New best model saved with mean reward: -2.12\n",
      "Episode 680, Reward: -71.90, Mean (100): -5.51\n",
      "Episode 690, Reward: 1.84, Mean (100): -5.47\n",
      "New best model saved with mean reward: -1.24\n",
      "New best model saved with mean reward: -0.79\n",
      "New best model saved with mean reward: 0.10\n",
      "New best model saved with mean reward: 0.71\n",
      "Episode 700, Reward: 17.53, Mean (100): 0.71\n",
      "New best model saved with mean reward: 1.36\n",
      "New best model saved with mean reward: 1.76\n",
      "New best model saved with mean reward: 2.54\n",
      "New best model saved with mean reward: 2.91\n",
      "New best model saved with mean reward: 3.50\n",
      "New best model saved with mean reward: 3.94\n",
      "New best model saved with mean reward: 4.13\n",
      "New best model saved with mean reward: 6.62\n",
      "New best model saved with mean reward: 7.63\n",
      "Episode 710, Reward: 136.83, Mean (100): 7.63\n",
      "New best model saved with mean reward: 10.75\n",
      "New best model saved with mean reward: 11.39\n",
      "New best model saved with mean reward: 11.78\n",
      "New best model saved with mean reward: 11.97\n",
      "New best model saved with mean reward: 12.38\n",
      "New best model saved with mean reward: 12.76\n",
      "New best model saved with mean reward: 12.90\n",
      "Episode 720, Reward: 46.57, Mean (100): 12.90\n",
      "New best model saved with mean reward: 14.13\n",
      "New best model saved with mean reward: 16.89\n",
      "New best model saved with mean reward: 19.09\n",
      "New best model saved with mean reward: 21.11\n",
      "New best model saved with mean reward: 21.51\n",
      "New best model saved with mean reward: 22.26\n",
      "New best model saved with mean reward: 23.85\n",
      "New best model saved with mean reward: 23.87\n",
      "Episode 730, Reward: -35.60, Mean (100): 23.87\n",
      "New best model saved with mean reward: 26.25\n",
      "New best model saved with mean reward: 29.21\n",
      "New best model saved with mean reward: 30.95\n",
      "New best model saved with mean reward: 31.19\n",
      "New best model saved with mean reward: 33.00\n",
      "Episode 740, Reward: 220.19, Mean (100): 33.00\n",
      "New best model saved with mean reward: 33.37\n",
      "New best model saved with mean reward: 34.08\n",
      "New best model saved with mean reward: 35.43\n",
      "New best model saved with mean reward: 36.33\n",
      "Episode 750, Reward: -238.74, Mean (100): 32.67\n",
      "Episode 760, Reward: -90.24, Mean (100): 25.45\n",
      "Episode 770, Reward: -275.64, Mean (100): 13.63\n",
      "Episode 780, Reward: -213.90, Mean (100): 16.59\n",
      "Episode 790, Reward: -105.55, Mean (100): 22.66\n",
      "Episode 800, Reward: -41.40, Mean (100): 29.37\n",
      "Episode 810, Reward: 65.68, Mean (100): 32.05\n",
      "New best model saved with mean reward: 36.84\n",
      "New best model saved with mean reward: 38.50\n",
      "Episode 820, Reward: 237.99, Mean (100): 38.50\n",
      "New best model saved with mean reward: 39.96\n",
      "New best model saved with mean reward: 40.10\n",
      "New best model saved with mean reward: 41.61\n",
      "New best model saved with mean reward: 41.69\n",
      "Episode 830, Reward: 271.77, Mean (100): 40.95\n",
      "Episode 840, Reward: -25.18, Mean (100): 37.51\n",
      "New best model saved with mean reward: 41.85\n",
      "Episode 850, Reward: -74.19, Mean (100): 41.81\n",
      "New best model saved with mean reward: 44.18\n",
      "New best model saved with mean reward: 44.70\n",
      "New best model saved with mean reward: 44.72\n",
      "New best model saved with mean reward: 44.89\n",
      "New best model saved with mean reward: 47.70\n",
      "New best model saved with mean reward: 50.35\n",
      "New best model saved with mean reward: 52.02\n",
      "New best model saved with mean reward: 54.95\n",
      "Episode 860, Reward: 203.23, Mean (100): 54.95\n",
      "New best model saved with mean reward: 55.69\n",
      "New best model saved with mean reward: 57.96\n",
      "New best model saved with mean reward: 60.45\n",
      "New best model saved with mean reward: 63.00\n",
      "New best model saved with mean reward: 63.50\n",
      "New best model saved with mean reward: 64.48\n",
      "New best model saved with mean reward: 67.61\n",
      "New best model saved with mean reward: 68.75\n",
      "New best model saved with mean reward: 73.24\n",
      "Episode 870, Reward: 173.86, Mean (100): 73.24\n",
      "Episode 880, Reward: 170.86, Mean (100): 73.01\n",
      "New best model saved with mean reward: 73.84\n",
      "Episode 890, Reward: 187.21, Mean (100): 71.83\n",
      "Episode 900, Reward: -31.06, Mean (100): 64.47\n",
      "Episode 910, Reward: -102.37, Mean (100): 60.18\n",
      "Episode 920, Reward: 45.39, Mean (100): 61.28\n",
      "Episode 930, Reward: 173.89, Mean (100): 55.59\n",
      "Episode 940, Reward: -5.00, Mean (100): 51.51\n",
      "Episode 950, Reward: -30.78, Mean (100): 50.57\n",
      "Episode 960, Reward: -66.16, Mean (100): 40.75\n",
      "Episode 970, Reward: -20.58, Mean (100): 28.30\n",
      "Episode 980, Reward: -32.34, Mean (100): 17.93\n",
      "Episode 990, Reward: -71.29, Mean (100): 4.91\n",
      "Episode 1000, Reward: -93.57, Mean (100): -8.34\n",
      "Episode 1010, Reward: -112.26, Mean (100): -19.33\n",
      "Episode 1020, Reward: -14.48, Mean (100): -37.43\n",
      "Episode 1030, Reward: -107.67, Mean (100): -51.86\n",
      "Episode 1040, Reward: 182.54, Mean (100): -52.85\n",
      "Episode 1050, Reward: 235.64, Mean (100): -39.10\n",
      "Episode 1060, Reward: 83.56, Mean (100): -29.12\n",
      "Episode 1070, Reward: 35.98, Mean (100): -14.73\n",
      "Episode 1080, Reward: 132.78, Mean (100): 0.33\n",
      "Episode 1090, Reward: 31.43, Mean (100): 10.08\n",
      "Episode 1100, Reward: 142.03, Mean (100): 30.21\n",
      "Episode 1110, Reward: -22.40, Mean (100): 49.72\n",
      "Episode 1120, Reward: 9.84, Mean (100): 68.69\n",
      "New best model saved with mean reward: 74.20\n",
      "New best model saved with mean reward: 75.34\n",
      "New best model saved with mean reward: 77.15\n",
      "New best model saved with mean reward: 78.29\n",
      "New best model saved with mean reward: 79.97\n",
      "New best model saved with mean reward: 81.02\n",
      "New best model saved with mean reward: 83.05\n",
      "New best model saved with mean reward: 83.29\n",
      "New best model saved with mean reward: 84.88\n",
      "Episode 1130, Reward: 50.89, Mean (100): 84.88\n",
      "New best model saved with mean reward: 86.03\n",
      "New best model saved with mean reward: 87.15\n",
      "New best model saved with mean reward: 89.27\n",
      "Episode 1140, Reward: 38.63, Mean (100): 87.44\n",
      "New best model saved with mean reward: 90.23\n",
      "Episode 1150, Reward: -8.68, Mean (100): 83.34\n",
      "Episode 1160, Reward: 137.15, Mean (100): 86.76\n",
      "Episode 1170, Reward: 158.94, Mean (100): 89.73\n",
      "New best model saved with mean reward: 90.84\n",
      "New best model saved with mean reward: 91.58\n",
      "New best model saved with mean reward: 92.44\n",
      "Episode 1180, Reward: 225.87, Mean (100): 92.44\n",
      "New best model saved with mean reward: 93.12\n",
      "New best model saved with mean reward: 95.86\n",
      "New best model saved with mean reward: 97.35\n",
      "Episode 1190, Reward: 28.22, Mean (100): 96.84\n",
      "New best model saved with mean reward: 97.49\n",
      "Episode 1200, Reward: 116.61, Mean (100): 96.99\n",
      "Episode 1210, Reward: 221.15, Mean (100): 95.40\n",
      "New best model saved with mean reward: 98.72\n",
      "Episode 1220, Reward: -173.56, Mean (100): 91.20\n",
      "Episode 1230, Reward: 232.42, Mean (100): 97.42\n",
      "New best model saved with mean reward: 98.97\n",
      "New best model saved with mean reward: 100.00\n",
      "New best model saved with mean reward: 101.52\n",
      "New best model saved with mean reward: 102.85\n",
      "New best model saved with mean reward: 106.06\n",
      "New best model saved with mean reward: 106.22\n",
      "New best model saved with mean reward: 107.31\n",
      "Episode 1240, Reward: 268.11, Mean (100): 107.31\n",
      "New best model saved with mean reward: 108.31\n",
      "New best model saved with mean reward: 109.77\n",
      "New best model saved with mean reward: 113.05\n",
      "New best model saved with mean reward: 113.47\n",
      "Episode 1250, Reward: 22.55, Mean (100): 108.52\n",
      "New best model saved with mean reward: 113.87\n",
      "New best model saved with mean reward: 114.78\n",
      "Episode 1260, Reward: 127.87, Mean (100): 114.13\n",
      "New best model saved with mean reward: 115.32\n",
      "New best model saved with mean reward: 116.25\n",
      "New best model saved with mean reward: 117.23\n",
      "New best model saved with mean reward: 117.30\n",
      "Episode 1270, Reward: 80.52, Mean (100): 115.48\n",
      "New best model saved with mean reward: 119.04\n",
      "New best model saved with mean reward: 119.97\n",
      "New best model saved with mean reward: 120.80\n",
      "Episode 1280, Reward: 234.35, Mean (100): 119.23\n",
      "New best model saved with mean reward: 121.58\n",
      "New best model saved with mean reward: 121.64\n",
      "New best model saved with mean reward: 121.87\n",
      "Episode 1290, Reward: 101.91, Mean (100): 121.87\n",
      "New best model saved with mean reward: 124.32\n",
      "New best model saved with mean reward: 125.44\n",
      "New best model saved with mean reward: 127.98\n",
      "Episode 1300, Reward: 89.78, Mean (100): 126.85\n",
      "New best model saved with mean reward: 128.96\n",
      "New best model saved with mean reward: 129.45\n",
      "New best model saved with mean reward: 131.76\n",
      "New best model saved with mean reward: 132.57\n",
      "New best model saved with mean reward: 133.38\n",
      "Episode 1310, Reward: 301.34, Mean (100): 133.38\n",
      "New best model saved with mean reward: 133.55\n",
      "New best model saved with mean reward: 133.79\n",
      "New best model saved with mean reward: 134.33\n",
      "New best model saved with mean reward: 134.98\n",
      "New best model saved with mean reward: 135.52\n",
      "New best model saved with mean reward: 139.67\n",
      "Episode 1320, Reward: 240.75, Mean (100): 139.67\n",
      "Episode 1330, Reward: -6.48, Mean (100): 131.31\n",
      "Episode 1340, Reward: 17.81, Mean (100): 123.39\n",
      "Episode 1350, Reward: 28.97, Mean (100): 127.31\n",
      "Episode 1360, Reward: -10.35, Mean (100): 119.51\n",
      "Episode 1370, Reward: 178.05, Mean (100): 117.91\n",
      "Episode 1380, Reward: 131.62, Mean (100): 108.77\n",
      "Episode 1390, Reward: 142.68, Mean (100): 104.46\n",
      "Episode 1400, Reward: 118.18, Mean (100): 99.35\n",
      "Episode 1410, Reward: 114.95, Mean (100): 95.73\n",
      "Episode 1420, Reward: 24.57, Mean (100): 96.02\n",
      "Episode 1430, Reward: 123.06, Mean (100): 109.75\n",
      "Episode 1440, Reward: 79.40, Mean (100): 118.08\n",
      "Episode 1450, Reward: 131.48, Mean (100): 119.66\n",
      "Episode 1460, Reward: 152.34, Mean (100): 125.53\n",
      "Episode 1470, Reward: 173.57, Mean (100): 120.93\n",
      "Episode 1480, Reward: -24.67, Mean (100): 120.19\n",
      "Episode 1490, Reward: 91.96, Mean (100): 122.00\n",
      "Episode 1500, Reward: 170.71, Mean (100): 123.31\n",
      "Episode 1510, Reward: 199.10, Mean (100): 126.37\n",
      "Episode 1520, Reward: -26.52, Mean (100): 123.53\n",
      "Episode 1530, Reward: 121.92, Mean (100): 117.02\n",
      "Episode 1540, Reward: 168.33, Mean (100): 116.79\n",
      "Episode 1550, Reward: 18.92, Mean (100): 116.84\n",
      "Episode 1560, Reward: 80.72, Mean (100): 118.92\n",
      "Episode 1570, Reward: 132.15, Mean (100): 119.96\n",
      "Episode 1580, Reward: 173.80, Mean (100): 125.33\n",
      "Episode 1590, Reward: 125.55, Mean (100): 125.42\n",
      "Episode 1600, Reward: 150.07, Mean (100): 128.47\n",
      "Episode 1610, Reward: 254.32, Mean (100): 126.86\n",
      "Episode 1620, Reward: 149.56, Mean (100): 127.08\n",
      "Episode 1630, Reward: 45.65, Mean (100): 124.60\n",
      "Episode 1640, Reward: 153.66, Mean (100): 124.94\n",
      "Episode 1650, Reward: 106.74, Mean (100): 126.17\n",
      "Episode 1660, Reward: 117.94, Mean (100): 126.34\n",
      "Episode 1670, Reward: 243.33, Mean (100): 132.94\n",
      "Episode 1680, Reward: 270.00, Mean (100): 135.81\n",
      "New best model saved with mean reward: 142.01\n",
      "New best model saved with mean reward: 142.29\n",
      "New best model saved with mean reward: 144.25\n",
      "New best model saved with mean reward: 145.44\n",
      "New best model saved with mean reward: 146.36\n",
      "New best model saved with mean reward: 147.31\n",
      "New best model saved with mean reward: 148.66\n",
      "Episode 1690, Reward: 260.29, Mean (100): 148.66\n",
      "New best model saved with mean reward: 150.63\n",
      "Episode 1700, Reward: 269.07, Mean (100): 148.32\n",
      "New best model saved with mean reward: 150.71\n",
      "New best model saved with mean reward: 151.05\n",
      "New best model saved with mean reward: 151.66\n",
      "New best model saved with mean reward: 152.18\n",
      "Episode 1710, Reward: 111.96, Mean (100): 150.75\n",
      "New best model saved with mean reward: 153.05\n",
      "New best model saved with mean reward: 153.58\n",
      "New best model saved with mean reward: 154.48\n",
      "New best model saved with mean reward: 155.36\n",
      "New best model saved with mean reward: 156.58\n",
      "Episode 1720, Reward: 271.98, Mean (100): 156.58\n",
      "New best model saved with mean reward: 159.10\n",
      "New best model saved with mean reward: 159.14\n",
      "New best model saved with mean reward: 161.02\n",
      "New best model saved with mean reward: 161.38\n",
      "New best model saved with mean reward: 162.89\n",
      "New best model saved with mean reward: 164.41\n",
      "New best model saved with mean reward: 165.21\n",
      "Episode 1730, Reward: 125.69, Mean (100): 165.21\n",
      "New best model saved with mean reward: 165.97\n",
      "New best model saved with mean reward: 166.24\n",
      "New best model saved with mean reward: 167.59\n",
      "Episode 1740, Reward: 123.22, Mean (100): 165.93\n",
      "Episode 1750, Reward: -11.11, Mean (100): 160.59\n",
      "Episode 1760, Reward: 86.86, Mean (100): 156.61\n",
      "Episode 1770, Reward: 149.14, Mean (100): 155.60\n",
      "Episode 1780, Reward: 111.51, Mean (100): 155.99\n",
      "Episode 1790, Reward: 112.99, Mean (100): 147.52\n",
      "Episode 1800, Reward: 164.62, Mean (100): 146.02\n",
      "Episode 1810, Reward: 216.46, Mean (100): 145.61\n",
      "Episode 1820, Reward: 241.82, Mean (100): 138.97\n",
      "Episode 1830, Reward: 215.30, Mean (100): 131.56\n",
      "Episode 1840, Reward: 113.06, Mean (100): 126.89\n",
      "Episode 1850, Reward: 175.91, Mean (100): 131.78\n",
      "Episode 1860, Reward: 204.27, Mean (100): 131.77\n",
      "Episode 1870, Reward: 151.02, Mean (100): 129.66\n",
      "Episode 1880, Reward: 83.45, Mean (100): 126.11\n",
      "Episode 1890, Reward: 177.37, Mean (100): 126.46\n",
      "Episode 1900, Reward: 214.26, Mean (100): 125.87\n",
      "Episode 1910, Reward: 142.77, Mean (100): 123.62\n",
      "Episode 1920, Reward: 126.48, Mean (100): 126.24\n",
      "Episode 1930, Reward: 215.87, Mean (100): 128.90\n",
      "Episode 1940, Reward: 104.44, Mean (100): 127.90\n",
      "Episode 1950, Reward: 91.31, Mean (100): 127.12\n",
      "Episode 1960, Reward: 156.78, Mean (100): 132.34\n",
      "Episode 1970, Reward: 118.77, Mean (100): 129.88\n",
      "Episode 1980, Reward: -5.74, Mean (100): 131.28\n",
      "Episode 1990, Reward: 87.48, Mean (100): 126.28\n",
      "Episode 2000, Reward: 231.46, Mean (100): 129.48\n",
      "Episode 2010, Reward: 224.47, Mean (100): 134.12\n",
      "Episode 2020, Reward: 238.67, Mean (100): 136.13\n",
      "Episode 2030, Reward: 201.04, Mean (100): 136.55\n",
      "Episode 2040, Reward: 275.76, Mean (100): 146.48\n",
      "Episode 2050, Reward: 188.55, Mean (100): 147.93\n",
      "Episode 2060, Reward: 221.50, Mean (100): 143.76\n",
      "Episode 2070, Reward: 246.73, Mean (100): 145.40\n",
      "Episode 2080, Reward: 180.44, Mean (100): 149.25\n",
      "Episode 2090, Reward: 205.06, Mean (100): 153.51\n",
      "Episode 2100, Reward: 140.77, Mean (100): 147.31\n",
      "Episode 2110, Reward: 73.32, Mean (100): 134.68\n",
      "Episode 2120, Reward: 213.65, Mean (100): 136.71\n",
      "Episode 2130, Reward: 160.96, Mean (100): 130.23\n",
      "Episode 2140, Reward: 198.54, Mean (100): 119.81\n",
      "Episode 2150, Reward: 236.42, Mean (100): 118.01\n",
      "Episode 2160, Reward: -2.79, Mean (100): 120.46\n",
      "Episode 2170, Reward: 223.70, Mean (100): 126.69\n",
      "Episode 2180, Reward: 190.38, Mean (100): 130.10\n",
      "Episode 2190, Reward: 180.14, Mean (100): 138.36\n",
      "Episode 2200, Reward: 9.53, Mean (100): 143.59\n",
      "Episode 2210, Reward: 155.54, Mean (100): 154.21\n",
      "Episode 2220, Reward: 227.12, Mean (100): 154.11\n",
      "Episode 2230, Reward: 223.63, Mean (100): 165.80\n",
      "New best model saved with mean reward: 168.35\n",
      "New best model saved with mean reward: 169.10\n",
      "New best model saved with mean reward: 171.22\n",
      "New best model saved with mean reward: 173.46\n",
      "New best model saved with mean reward: 173.53\n",
      "Episode 2240, Reward: 205.21, Mean (100): 173.53\n",
      "New best model saved with mean reward: 175.54\n",
      "New best model saved with mean reward: 177.45\n",
      "New best model saved with mean reward: 179.91\n",
      "New best model saved with mean reward: 181.65\n",
      "Episode 2250, Reward: 239.41, Mean (100): 179.04\n",
      "Episode 2260, Reward: 237.43, Mean (100): 180.03\n",
      "New best model saved with mean reward: 182.00\n",
      "New best model saved with mean reward: 182.38\n",
      "Episode 2270, Reward: 209.23, Mean (100): 179.20\n",
      "Episode 2280, Reward: 163.01, Mean (100): 173.79\n",
      "Episode 2290, Reward: 75.22, Mean (100): 166.42\n",
      "Episode 2300, Reward: 145.43, Mean (100): 164.03\n",
      "Episode 2310, Reward: 185.92, Mean (100): 164.57\n",
      "Episode 2320, Reward: 233.88, Mean (100): 163.17\n",
      "Episode 2330, Reward: -50.27, Mean (100): 153.19\n",
      "Episode 2340, Reward: -80.86, Mean (100): 151.96\n",
      "Episode 2350, Reward: 202.74, Mean (100): 141.11\n",
      "Episode 2360, Reward: 3.24, Mean (100): 135.46\n",
      "Episode 2370, Reward: -160.38, Mean (100): 131.80\n",
      "Episode 2380, Reward: 203.76, Mean (100): 127.60\n",
      "Episode 2390, Reward: 89.09, Mean (100): 127.43\n",
      "Episode 2400, Reward: 51.00, Mean (100): 126.40\n",
      "Episode 2410, Reward: 160.71, Mean (100): 123.58\n",
      "Episode 2420, Reward: 55.17, Mean (100): 121.57\n",
      "Episode 2430, Reward: 230.82, Mean (100): 121.68\n",
      "Episode 2440, Reward: 98.79, Mean (100): 118.09\n",
      "Episode 2450, Reward: 2.20, Mean (100): 121.40\n",
      "Episode 2460, Reward: 58.13, Mean (100): 120.99\n",
      "Episode 2470, Reward: 187.27, Mean (100): 118.31\n",
      "Episode 2480, Reward: 46.56, Mean (100): 117.83\n",
      "Episode 2490, Reward: 43.38, Mean (100): 106.25\n",
      "Episode 2500, Reward: 23.56, Mean (100): 97.82\n",
      "Episode 2510, Reward: -3.28, Mean (100): 84.68\n",
      "Episode 2520, Reward: 23.11, Mean (100): 72.07\n",
      "Episode 2530, Reward: 12.00, Mean (100): 65.28\n",
      "Episode 2540, Reward: 5.94, Mean (100): 54.76\n",
      "Episode 2550, Reward: 15.21, Mean (100): 46.55\n",
      "Episode 2560, Reward: 15.52, Mean (100): 37.78\n",
      "Episode 2570, Reward: 40.74, Mean (100): 30.14\n",
      "Episode 2580, Reward: 5.58, Mean (100): 21.78\n",
      "Episode 2590, Reward: 3.39, Mean (100): 24.81\n",
      "Episode 2600, Reward: 47.03, Mean (100): 27.59\n",
      "Episode 2610, Reward: 9.15, Mean (100): 32.48\n",
      "Episode 2620, Reward: 27.05, Mean (100): 33.83\n",
      "Episode 2630, Reward: 229.20, Mean (100): 33.61\n",
      "Episode 2640, Reward: 42.01, Mean (100): 38.11\n",
      "Episode 2650, Reward: 36.32, Mean (100): 38.32\n",
      "Episode 2660, Reward: 216.48, Mean (100): 42.68\n",
      "Episode 2670, Reward: 17.50, Mean (100): 46.77\n",
      "Episode 2680, Reward: -6.45, Mean (100): 48.04\n",
      "Episode 2690, Reward: 47.71, Mean (100): 47.62\n",
      "Episode 2700, Reward: 28.72, Mean (100): 51.41\n",
      "Episode 2710, Reward: 43.91, Mean (100): 52.27\n",
      "Episode 2720, Reward: 16.70, Mean (100): 57.03\n",
      "Episode 2730, Reward: 162.81, Mean (100): 67.08\n",
      "Episode 2740, Reward: -96.53, Mean (100): 63.62\n",
      "Episode 2750, Reward: -63.76, Mean (100): 56.12\n",
      "Episode 2760, Reward: -33.48, Mean (100): 44.02\n",
      "Episode 2770, Reward: 217.29, Mean (100): 40.83\n",
      "Episode 2780, Reward: 232.04, Mean (100): 55.89\n",
      "Episode 2790, Reward: 118.12, Mean (100): 65.64\n",
      "Episode 2800, Reward: 214.35, Mean (100): 68.35\n",
      "Episode 2810, Reward: 144.07, Mean (100): 77.53\n",
      "Episode 2820, Reward: 137.75, Mean (100): 87.47\n",
      "Episode 2830, Reward: 156.52, Mean (100): 90.41\n",
      "Episode 2840, Reward: 101.67, Mean (100): 102.58\n",
      "Episode 2850, Reward: 248.05, Mean (100): 121.94\n",
      "Episode 2860, Reward: 121.94, Mean (100): 141.87\n",
      "Episode 2870, Reward: 58.40, Mean (100): 144.30\n",
      "Episode 2880, Reward: -13.07, Mean (100): 142.22\n",
      "Episode 2890, Reward: 178.89, Mean (100): 140.98\n",
      "Episode 2900, Reward: 38.91, Mean (100): 142.79\n",
      "Episode 2910, Reward: 115.25, Mean (100): 146.71\n",
      "Episode 2920, Reward: 207.91, Mean (100): 145.17\n",
      "Episode 2930, Reward: 97.85, Mean (100): 136.05\n",
      "Episode 2940, Reward: 23.06, Mean (100): 129.49\n",
      "Episode 2950, Reward: 213.96, Mean (100): 125.02\n",
      "Episode 2960, Reward: 152.25, Mean (100): 120.52\n",
      "Episode 2970, Reward: 202.93, Mean (100): 127.10\n",
      "Episode 2980, Reward: 91.72, Mean (100): 114.96\n",
      "Episode 2990, Reward: 82.38, Mean (100): 105.89\n",
      "Episode 3000, Reward: -27.93, Mean (100): 92.15\n",
      "Training completed. Final model saved.\n",
      "\n",
      "Evaluating best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oskar\\AppData\\Local\\Temp\\ipykernel_19656\\3228179180.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.policy.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation over 100 episodes:\n",
      "Average Reward: 105.22 ± 79.38\n",
      "Average Episode Length: 795.34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import csv\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Policy, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        self.input_dim = env.observation_space.shape[0]\n",
    "        self.output_dim = env.action_space.n\n",
    "        self.policy = Policy(self.input_dim, self.output_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy.to(self.device)\n",
    "    \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        if deterministic:\n",
    "            return torch.argmax(probs).item()\n",
    "        \n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def update_policy(self, rewards, log_probs):\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        gamma = 0.99\n",
    "        \n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "        \n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards).to(self.device)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        self.policy.to(self.device)\n",
    "\n",
    "def evaluate(agent, env, num_episodes=100, max_steps=1000):\n",
    "    all_rewards = []\n",
    "    all_steps = []\n",
    "    \n",
    "    with open('reinforce_evaluation_results.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Episode', 'Reward', 'Steps'])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        all_steps.append(step + 1)\n",
    "        \n",
    "        with open('reinforce_evaluation_results.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([i_episode + 1, episode_reward, step + 1])\n",
    "    \n",
    "    avg_reward = np.mean(all_rewards)\n",
    "    avg_steps = np.mean(all_steps)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    \n",
    "    print(f\"\\nEvaluation over {num_episodes} episodes:\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "    print(f\"Average Episode Length: {avg_steps:.2f}\")\n",
    "    \n",
    "    return avg_reward, avg_steps\n",
    "\n",
    "def train():\n",
    "    with open('reinforce_training_rewards.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Episode', 'Reward', 'Mean_100'])\n",
    "    \n",
    "    env = gym.make(\"LunarLander-v3\")\n",
    "    \n",
    "    agent = REINFORCE(env, learning_rate=1e-3)\n",
    "    \n",
    "    num_episodes = 3000  \n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    best_mean_reward = -float('inf')\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        policy_loss = agent.update_policy(rewards, log_probs)\n",
    "        total_reward = sum(rewards)\n",
    "        recent_rewards.append(total_reward)\n",
    "        \n",
    "        mean_reward = np.mean(list(recent_rewards))\n",
    "        \n",
    "        with open('reinforce_training_rewards.csv', 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([episode + 1, total_reward, mean_reward])\n",
    "        \n",
    "        if mean_reward > best_mean_reward and len(recent_rewards) == 100:\n",
    "            best_mean_reward = mean_reward\n",
    "            agent.save('best_reinforce_model.pt')\n",
    "            print(f\"New best model saved with mean reward: {best_mean_reward:.2f}\")\n",
    "        \n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Episode {episode + 1}, Reward: {total_reward:.2f}, Mean (100): {mean_reward:.2f}\")\n",
    "    \n",
    "    agent.save('final_reinforce_model.pt')\n",
    "    print(\"Training completed. Final model saved.\")\n",
    "    \n",
    "    print(\"\\nEvaluating best model...\")\n",
    "    agent.load('best_reinforce_model.pt')\n",
    "    evaluate(agent, env)\n",
    "    \n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
